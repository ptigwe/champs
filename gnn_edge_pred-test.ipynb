{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric(df, preds):\n",
    "    df[\"prediction\"] = preds\n",
    "    maes = []\n",
    "    for t in df.type.unique():\n",
    "        y_true = df[df.type==t].scalar_coupling_constant.values\n",
    "        y_pred = df[df.type==t].prediction.values\n",
    "        mae = np.log(metrics.mean_absolute_error(y_true, y_pred))\n",
    "        maes.append(mae)\n",
    "    return np.mean(maes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_structures(df, struct_df, mulliken_df, potential):\n",
    "    groups = {}\n",
    "    struct_g = struct_df.groupby('molecule_name')\n",
    "    mulliken_g = mulliken_df.groupby('molecule_name')\n",
    "    for g, gdf in df.groupby('molecule_name'):\n",
    "        groups[g] = (gdf, struct_g.get_group(g), \n",
    "                     mulliken_g.get_group(g), \n",
    "                     potential[potential.molecule_name == g])\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "req_files = ['test.csv', 'structures.csv', 'mulliken_charges.csv', 'potential_energy.csv']\n",
    "def read_csvs(path):\n",
    "    read_csv = lambda x: pd.read_csv(osp.join(path, x))\n",
    "    return tuple(map(read_csv, req_files))\n",
    "\n",
    "def get_data_list(path):\n",
    "    train_df, structures, mulliken, potential = read_csvs(path)\n",
    "    train_df['type'] = train_df['type'].astype('category')\n",
    "    train_df['type_c'] = train_df['type'].cat.codes\n",
    "    structures['atom'] = structures['atom'].astype('category')\n",
    "    structures['atom_c'] = structures['atom'].cat.codes\n",
    "    return list(group_structures(train_df, structures, mulliken, potential).values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_data(first):\n",
    "    src, dst = first[0].atom_index_0, first[0].atom_index_1\n",
    "    src, dst = np.concatenate((src, dst)), np.concatenate((dst, src))\n",
    "    edge_idx = np.stack((src, dst))\n",
    "    scalar_coupling = np.zeros((edge_idx.shape[1], 1))#concatenate((first[0].scalar_coupling_constant, first[0].scalar_coupling_constant))\n",
    "    edge_types = np.concatenate((first[0].type_c.values, first[0].type_c.values))\n",
    "    xyz, atom = first[1].iloc[:,3:-1].values, first[1].iloc[:,-1].values\n",
    "    mul_charge = first[2].iloc[:,-1].values\n",
    "    print(first[3])\n",
    "    data = Data(pos=torch.FloatTensor(xyz), \n",
    "                edge_index=torch.LongTensor(edge_idx), \n",
    "                edge_types=torch.LongTensor(edge_types),\n",
    "                atom=torch.LongTensor(atom),\n",
    "                charge=torch.FloatTensor(mul_charge),\n",
    "                energy=torch.FloatTensor(first[3].potential_energy.values),\n",
    "                batch_edge_index=torch.zeros(edge_types.shape, dtype=torch.long),\n",
    "                scalar_coupling=torch.FloatTensor(scalar_coupling))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Complete(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, data):\n",
    "        complete_edges = np.array(list(itertools.permutations(range(data.num_nodes),2))).T\n",
    "        data.edge_index = torch.LongTensor(complete_edges)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Squeeze_Edge_Types(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, data):\n",
    "        data.edge_types = data.edge_types.squeeze()\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import InMemoryDataset, Data\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "class MyOwnDataset(InMemoryDataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None):\n",
    "        super(MyOwnDataset, self).__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "        \n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return ['structures.csv', 'mulliken_charges.csv', 'test.csv', 'magnetic_shielding_tensors.csv']\n",
    "    \n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['data_test.pt']\n",
    "    \n",
    "    def _download(self):\n",
    "        pass\n",
    "    \n",
    "    def process(self):\n",
    "        data_list = get_data_list(self.root)\n",
    "        data_list = [to_data(data) for data in data_list]\n",
    "        \n",
    "        if self.pre_filter is not None:\n",
    "            data_list = [data for data in data_list if self.pre_filter(data)]\n",
    "        \n",
    "        if self.pre_transform is not None:\n",
    "            data_list = [self.pre_transform(data) for data in data_list]\n",
    "            \n",
    "        data, slices = self.collate(data_list)\n",
    "        torch.save((data, slices), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm data/processed/data.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MyOwnDataset('data', transform=T.Compose([T.Distance()]))#, transform=Complete())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]['scalar_coupling']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = len(dataset)\n",
    "train = int(.5 * size)\n",
    "valid = int(.3 * size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize targets to mean=0 and std=1\n",
    "mean = dataset.data.energy.mean(dim=0, keepdim=True)\n",
    "std = dataset.data.energy.std(dim=0, keepdim=True)\n",
    "dataset.data.energy = (dataset.data.energy - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize targets to mean=0 and std=1\n",
    "mean = dataset.data.scalar_coupling.mean(dim=0, keepdim=True)\n",
    "std = dataset.data.scalar_coupling.std(dim=0, keepdim=True)\n",
    "dataset.data.scalar_coupling = (dataset.data.scalar_coupling - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, std = (torch.tensor([15.9159]), torch.tensor([34.9347]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask = torch.FloatTensor(len(dataset)).uniform_() > 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask.sum() / float(train_mask.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset[train_mask]\n",
    "valid_dataset = dataset[~train_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import NNConv, Set2Set, GCNConv\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "from torch.nn import Sequential, Linear, ReLU, GRU, Embedding, LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 64\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.lin0 = torch.nn.Linear(10, dim)\n",
    "        \n",
    "        self.pos_emb = Linear(3, 5)\n",
    "        self.atom_emb = Embedding(5, 5)\n",
    "        self.edge_emb = Embedding(8, 5)\n",
    "        self.dist_emb = Linear(1, 5)\n",
    "        nn = Sequential(Linear(10, 128), LeakyReLU(0.1), Linear(128, dim * dim))\n",
    "        self.conv = NNConv(dim, dim, nn, aggr='mean')\n",
    "        self.gru = GRU(dim, dim)\n",
    "\n",
    "        self.set2set = Set2Set(dim, processing_steps=6)\n",
    "        self.lin1 = torch.nn.Linear(2 * dim, dim)\n",
    "        self.lin2 = torch.nn.Linear(dim, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        pos = self.pos_emb(data.pos)\n",
    "        atom_emb = self.atom_emb(data.atom)\n",
    "        x = torch.cat((pos, atom_emb), dim=1)\n",
    "        out = F.relu(self.lin0(x))\n",
    "        h = out.unsqueeze(0)\n",
    "        \n",
    "        edge_emb = self.edge_emb(data.edge_types)\n",
    "        edge_dist = self.dist_emb(data.edge_attr)\n",
    "        edge_attr = torch.cat((edge_emb, edge_dist), dim=1)\n",
    "\n",
    "        for i in range(6):\n",
    "            m = F.relu(self.conv(out, data.edge_index, edge_attr))\n",
    "            out, h = self.gru(m.unsqueeze(0), h)\n",
    "            out = out.squeeze(0)\n",
    "            \n",
    "        x = torch.index_select(out, 0, data.edge_index.T.contiguous().view(-1))\n",
    "        x = x.view((data.edge_index.shape[1], -1))\n",
    "\n",
    "        #out = self.set2set(out, data.batch)\n",
    "        out = F.relu(self.lin1(x))\n",
    "        out = self.lin2(out)\n",
    "        return out.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.batch_edge_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net(b).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = torch.load('model_0.0360_0.03757.pt', map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "import tqdm\n",
    "with torch.no_grad():\n",
    "    for b in tqdm.tqdm(train_loader):\n",
    "        r = n(b).view(2, -1).mean(dim=0)\n",
    "        res.append(r)\n",
    "        #print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_ = torch.cat(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = res_.cpu().detach() * std + mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/test.csv')\n",
    "df.shape, r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['scalar_coupling_constant'] = r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['id', 'scalar_coupling_constant']].to_csv('sample_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.molecule_name.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l head data/sample_submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv /home/tobenna/Downloads/test.csv.zip .\n",
    "!unzip test.csv.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l data/test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = pd.read_csv('data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.merge(df).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
