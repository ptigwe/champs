{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric(df, preds):\n",
    "    df[\"prediction\"] = preds\n",
    "    maes = []\n",
    "    for t in df.type.unique():\n",
    "        y_true = df[df.type==t].scalar_coupling_constant.values\n",
    "        y_pred = df[df.type==t].prediction.values\n",
    "        mae = np.log(metrics.mean_absolute_error(y_true, y_pred))\n",
    "        maes.append(mae)\n",
    "    return np.mean(maes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groups all dataframes according to the molecule and returns\n",
    "# result as a dictionary\n",
    "def group_structures(df, struct_df, mulliken_df, potential):\n",
    "    groups = {}\n",
    "    struct_g = struct_df.groupby('molecule_name')\n",
    "    mulliken_g = mulliken_df.groupby('molecule_name')\n",
    "    for g, gdf in df.groupby('molecule_name'):\n",
    "        groups[g] = (gdf, struct_g.get_group(g), \n",
    "                     mulliken_g.get_group(g), \n",
    "                     potential[potential.molecule_name == g])\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "req_files = ['train.csv', 'structures.csv', 'mulliken_charges.csv', 'potential_energy.csv']\n",
    "def read_csvs(path):\n",
    "    read_csv = lambda x: pd.read_csv(osp.join(path, x))\n",
    "    return tuple(map(read_csv, req_files))\n",
    "\n",
    "# Reads files from path and returns a zipped list of the data according\n",
    "# to the molecule\n",
    "def get_data_list(path):\n",
    "    train_df, structures, mulliken, potential = read_csvs(path)\n",
    "    train_df['type'] = train_df['type'].astype('category')\n",
    "    train_df['type_c'] = train_df['type'].cat.codes\n",
    "    structures['atom'] = structures['atom'].astype('category')\n",
    "    structures['atom_c'] = structures['atom'].cat.codes\n",
    "    return list(group_structures(train_df, structures, mulliken, potential).values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_data(first):\n",
    "    # Edge Attributes\n",
    "    # Edges are bidirectional\n",
    "    src, dst = first[0].atom_index_0, first[0].atom_index_1\n",
    "    src, dst = np.concatenate((src, dst)), np.concatenate((dst, src))\n",
    "    edge_idx = np.stack((src, dst))\n",
    "    \n",
    "    scalar_coupling = first[0].scalar_coupling_constant.values\n",
    "    # Edge types\n",
    "    edge_types = np.concatenate((first[0].type_c.values, first[0].type_c.values))\n",
    "    \n",
    "    # Atom Attributes\n",
    "    xyz, atom = first[1].iloc[:,3:-1].values, first[1].iloc[:,-1].values\n",
    "    mul_charge = first[2].iloc[:,-1].values\n",
    "    \n",
    "    data = Data(pos=torch.FloatTensor(xyz), \n",
    "                edge_index=torch.LongTensor(edge_idx), \n",
    "                edge_types=torch.LongTensor(edge_types),\n",
    "                atom=torch.LongTensor(atom),\n",
    "                charge=torch.FloatTensor(mul_charge),\n",
    "                energy=torch.FloatTensor(first[3].potential_energy.values),\n",
    "                batch_edge_index=torch.zeros(edge_types.shape, dtype=torch.long),\n",
    "                half_edge_index=torch.LongTensor(edge_idx[:, :edge_idx.shape[1] // 2]),\n",
    "                scalar_coupling=torch.FloatTensor(scalar_coupling))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Given two numpy arrays A and B, returns A - B, considering columns\n",
    "# of A and B to be elements in a set\n",
    "def set_diff(A, B):\n",
    "    A, B = np.ascontiguousarray(A.T), np.ascontiguousarray(B.T)\n",
    "    nrows, ncols = A.shape\n",
    "    \n",
    "    dtype={'names':['f{}'.format(i) for i in range(ncols)],\n",
    "           'formats':ncols * [A.dtype]}\n",
    "    C = np.setdiff1d(A.view(dtype), B.view(dtype))\n",
    "    C = C.view(A.dtype).reshape(-1, ncols)\n",
    "    \n",
    "    #print(C.dtype)\n",
    "    return np.ascontiguousarray(C.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph transformation to make a graph a complete graph\n",
    "class Complete(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, data):\n",
    "        complete_edges = np.array(list(itertools.permutations(range(data.num_nodes),2))).T\n",
    "        # additional edges needed to make a complete graph\n",
    "        new_edges = torch.LongTensor(set_diff(complete_edges, data.edge_index)).contiguous()\n",
    "        \n",
    "        # label all additional edges as type 8 (UNK)\n",
    "        new_types = 8 * np.ones(new_edges.shape[1])\n",
    "        \n",
    "        data.prev_edge_index = data.edge_index\n",
    "        data.edge_index = torch.cat((data.edge_index, new_edges), axis=1)\n",
    "        \n",
    "        data.full_types = torch.LongTensor(np.concatenate((data.edge_types, new_types)))#.cuda()\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import InMemoryDataset, Data\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "class MyOwnDataset(InMemoryDataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None):\n",
    "        super(MyOwnDataset, self).__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "        \n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return ['structures.csv', 'mulliken_charges.csv', 'train.csv', 'magnetic_shielding_tensors.csv']\n",
    "    \n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['data_tmp.pt']\n",
    "    \n",
    "    def _download(self):\n",
    "        pass\n",
    "    \n",
    "    def process(self):\n",
    "        data_list = get_data_list(self.root)\n",
    "        data_list = [to_data(data) for data in data_list]\n",
    "        \n",
    "        if self.pre_filter is not None:\n",
    "            data_list = [data for data in data_list if self.pre_filter(data)]\n",
    "        \n",
    "        if self.pre_transform is not None:\n",
    "            data_list = [self.pre_transform(data) for data in data_list]\n",
    "            \n",
    "        data, slices = self.collate(data_list)\n",
    "        torch.save((data, slices), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm data/processed/data.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rm data/processed/data_tmp.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MyOwnDataset('data', transform=T.Compose([Complete(), T.Distance()]))#, transform=Complete())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize targets to mean=0 and std=1\n",
    "if normalize:\n",
    "    mean = dataset.data.energy.mean(dim=0, keepdim=True)\n",
    "    std = dataset.data.energy.std(dim=0, keepdim=True)\n",
    "    dataset.data.energy = (dataset.data.energy - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize targets to mean=0 and std=1\n",
    "if normalize:\n",
    "    mean = dataset.data.scalar_coupling.mean(dim=0, keepdim=True)\n",
    "    std = dataset.data.scalar_coupling.std(dim=0, keepdim=True)\n",
    "    dataset.data.scalar_coupling = (dataset.data.scalar_coupling - mean) / std\n",
    "    print(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(dataset.data.scalar_coupling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask = torch.FloatTensor(len(dataset)).uniform_() > 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask.sum() / float(train_mask.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset[train_mask]\n",
    "valid_dataset = dataset[~train_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import NNConv, Set2Set, GCNConv\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "from torch.nn import Sequential, Linear, ReLU, GRU, Embedding, LeakyReLU, BatchNorm1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=50, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=500, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 64\n",
    "POS_SIZE = 20\n",
    "ATOM_SIZE = 20\n",
    "EDGE_SIZE = 25\n",
    "DIST_SIZE = 20\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        def seq_emb(in_, out_, fl):\n",
    "            return Sequential(fl(in_, out_),\n",
    "                              LeakyReLU(),\n",
    "                              BatchNorm1d(out_))\n",
    "        \n",
    "        self.lin0 = torch.nn.Linear(POS_SIZE + ATOM_SIZE, dim)\n",
    "        \n",
    "        self.pos_emb = seq_emb(3, POS_SIZE, Linear)\n",
    "        self.atom_emb = seq_emb(5, ATOM_SIZE, Embedding)\n",
    "        self.edge_emb = seq_emb(9, EDGE_SIZE, Embedding) \n",
    "        self.dist_emb = seq_emb(1, DIST_SIZE, Linear)\n",
    "        nn = Sequential(Linear(EDGE_SIZE + DIST_SIZE, 128), LeakyReLU(0.1), BatchNorm1d(128), Linear(128, dim * dim))\n",
    "        self.conv = NNConv(dim, dim, nn, aggr='mean')\n",
    "        self.gru = GRU(dim, dim)\n",
    "\n",
    "        self.set2set = Set2Set(dim, processing_steps=6)\n",
    "        self.lin1 = torch.nn.Linear(2 * dim, dim)\n",
    "        self.lin2 = torch.nn.Linear(dim, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        pos = self.pos_emb(data.pos)\n",
    "        atom_emb = self.atom_emb(data.atom)\n",
    "        x = torch.cat((pos, atom_emb), dim=1)\n",
    "        out = F.relu(self.lin0(x))\n",
    "        h = out.unsqueeze(0)\n",
    "        \n",
    "        edge_emb = self.edge_emb(data.full_types)\n",
    "        edge_dist = self.dist_emb(data.edge_attr)\n",
    "        edge_attr = torch.cat((edge_emb, edge_dist), dim=1)\n",
    "\n",
    "        for i in range(6):\n",
    "            m = F.relu(self.conv(out, data.edge_index, edge_attr))\n",
    "            out, h = self.gru(m.unsqueeze(0), h)\n",
    "            out = out.squeeze(0)\n",
    "            \n",
    "        x = torch.index_select(out, 0, data.half_edge_index.T.contiguous().view(-1))\n",
    "        x = x.view((data.half_edge_index.shape[1], -1))\n",
    "\n",
    "        #out = self.set2set(out, data.batch)\n",
    "        out = F.relu(self.lin1(x))\n",
    "        out = self.lin2(out)\n",
    "        return out.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "class LightningNet(pl.LightningModule):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        self.net = Net()\n",
    "        self.lr = hparams.learning_rate\n",
    "        self.hparams = hparams\n",
    "        \n",
    "    def forward(self, data):\n",
    "        return self.net(data)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        output = self.forward(batch)\n",
    "        loss = torch.log(F.mse_loss(output, batch.scalar_coupling))\n",
    "        return {'loss': loss, 'progress_bar': {'training_loss': loss}}\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        output = self.forward(batch)\n",
    "        loss = torch.log(F.mse_loss(output, batch.scalar_coupling))\n",
    "        return {'loss': loss}\n",
    "    \n",
    "    def validation_end(self, outputs):\n",
    "        avg_loss = torch.mean(torch.Tensor([x['loss'] for x in outputs]))\n",
    "        return {'val_loss': avg_loss,\n",
    "                'progress_bar': {'valid_loss': avg_loss},\n",
    "                'log': {'valid_loss': avg_loss}}\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.AdamW(self.parameters())\n",
    "        lr_sch = torch.optim.lr_scheduler.OneCycleLR(optim, self.lr, \n",
    "                                                     len(train_loader),\n",
    "                                                     epochs=30)\n",
    "        return [optim], [lr_sch]\n",
    "    \n",
    "    @pl.data_loader\n",
    "    def train_dataloader(self):\n",
    "        return train_loader\n",
    "    \n",
    "    @pl.data_loader\n",
    "    def val_dataloader(self):\n",
    "        return valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "# usually these come from command line args\n",
    "args = Namespace(**{'learning_rate':0.001})\n",
    "net = LightningNet(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.half_edge_index[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.cpu()\n",
    "net(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop_callback = pl.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    strict=False,\n",
    "    verbose=False,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(gpus=1, \n",
    "                     early_stop_callback=early_stop_callback)\n",
    "trainer.fit(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = LightningNet.load_from_checkpoint('../kaggle/champs/lightning_logs/version_77/checkpoints/_ckpt_epoch_12.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lt lightning_logs/**/**/*.ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class DS(Dataset):\n",
    "    def __init__(self, ds):\n",
    "        self.ds = ds\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.ds[idx]\n",
    "        return x, x.scalar_coupling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crit = lambda y_pred, y: torch.log(F.mse_loss(y_pred, y))\n",
    "lrf = LRFinder(net, torch.optim.AdamW(net.parameters()), crit, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrf.range_test(DS(train_dataset), start_lr=1e-6, end_lr=0.001, num_iter=100)\n",
    "lrf.plot() # to inspect the loss-learning rate graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.load_from_checkpoint('first.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
