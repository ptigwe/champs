{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric(df, preds):\n",
    "    df[\"prediction\"] = preds\n",
    "    maes = []\n",
    "    for t in df.type.unique():\n",
    "        y_true = df[df.type==t].scalar_coupling_constant.values\n",
    "        y_pred = df[df.type==t].prediction.values\n",
    "        mae = np.log(metrics.mean_absolute_error(y_true, y_pred))\n",
    "        maes.append(mae)\n",
    "    return np.mean(maes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_structures(df, struct_df, mulliken_df, potential):\n",
    "    groups = {}\n",
    "    struct_g = struct_df.groupby('molecule_name')\n",
    "    mulliken_g = mulliken_df.groupby('molecule_name')\n",
    "    for g, gdf in df.groupby('molecule_name'):\n",
    "        groups[g] = (gdf, struct_g.get_group(g), \n",
    "                     mulliken_g.get_group(g), \n",
    "                     potential[potential.molecule_name == g])\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "req_files = ['train.csv', 'structures.csv', 'mulliken_charges.csv', 'potential_energy.csv']\n",
    "def read_csvs(path):\n",
    "    read_csv = lambda x: pd.read_csv(osp.join(path, x))\n",
    "    return tuple(map(read_csv, req_files))\n",
    "\n",
    "def get_data_list(path):\n",
    "    train_df, structures, mulliken, potential = read_csvs(path)\n",
    "    train_df['type'] = train_df['type'].astype('category')\n",
    "    train_df['type_c'] = train_df['type'].cat.codes\n",
    "    structures['atom'] = structures['atom'].astype('category')\n",
    "    structures['atom_c'] = structures['atom'].cat.codes\n",
    "    return list(group_structures(train_df, structures, mulliken, potential).values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_data(first):\n",
    "    src, dst = first[0].atom_index_0, first[0].atom_index_1\n",
    "    src, dst = np.concatenate((src, dst)), np.concatenate((dst, src))\n",
    "    edge_idx = np.stack((src, dst))\n",
    "    scalar_coupling = np.concatenate((first[0].scalar_coupling_constant, first[0].scalar_coupling_constant))\n",
    "    edge_types = np.concatenate((first[0].type_c.values, first[0].type_c.values))\n",
    "    xyz, atom = first[1].iloc[:,3:-1].values, first[1].iloc[:,-1].values\n",
    "    mul_charge = first[2].iloc[:,-1].values\n",
    "    print(first[3])\n",
    "    data = Data(pos=torch.FloatTensor(xyz), \n",
    "                edge_index=torch.LongTensor(edge_idx), \n",
    "                edge_types=torch.LongTensor(edge_types),\n",
    "                atom=torch.LongTensor(atom),\n",
    "                charge=torch.FloatTensor(mul_charge),\n",
    "                energy=torch.FloatTensor(first[3].potential_energy.values),\n",
    "                batch_edge_index=torch.zeros(edge_types.shape, dtype=torch.long),\n",
    "                scalar_coupling=torch.FloatTensor(scalar_coupling))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Complete(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, data):\n",
    "        complete_edges = np.array(list(itertools.permutations(range(data.num_nodes),2))).T\n",
    "        data.edge_index = torch.LongTensor(complete_edges)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Squeeze_Edge_Types(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, data):\n",
    "        data.edge_types = data.edge_types.squeeze()\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import InMemoryDataset, Data\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "class MyOwnDataset(InMemoryDataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None):\n",
    "        super(MyOwnDataset, self).__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "        \n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return ['structures.csv', 'mulliken_charges.csv', 'train.csv', 'magnetic_shielding_tensors.csv']\n",
    "    \n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['data.pt']\n",
    "    \n",
    "    def _download(self):\n",
    "        pass\n",
    "    \n",
    "    def process(self):\n",
    "        data_list = get_data_list(self.root)\n",
    "        data_list = [to_data(data) for data in data_list]\n",
    "        \n",
    "        if self.pre_filter is not None:\n",
    "            data_list = [data for data in data_list if self.pre_filter(data)]\n",
    "        \n",
    "        if self.pre_transform is not None:\n",
    "            data_list = [self.pre_transform(data) for data in data_list]\n",
    "            \n",
    "        data, slices = self.collate(data_list)\n",
    "        torch.save((data, slices), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm data/processed/data.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MyOwnDataset('data', transform=T.Compose([T.Distance()]))#, transform=Complete())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]['scalar_coupling']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = len(dataset)\n",
    "train = int(.5 * size)\n",
    "valid = int(.3 * size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize targets to mean=0 and std=1\n",
    "mean = dataset.data.energy.mean(dim=0, keepdim=True)\n",
    "std = dataset.data.energy.std(dim=0, keepdim=True)\n",
    "dataset.data.energy = (dataset.data.energy - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize targets to mean=0 and std=1\n",
    "mean = dataset.data.scalar_coupling.mean(dim=0, keepdim=True)\n",
    "std = dataset.data.scalar_coupling.std(dim=0, keepdim=True)\n",
    "dataset.data.scalar_coupling = (dataset.data.scalar_coupling - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(dataset.data.scalar_coupling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask = torch.FloatTensor(len(dataset)).uniform_() > 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask.sum() / float(train_mask.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset[train_mask]\n",
    "valid_dataset = dataset[~train_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import NNConv, Set2Set, GCNConv\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "from torch.nn import Sequential, Linear, ReLU, GRU, Embedding, LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=100, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 64\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.lin0 = torch.nn.Linear(10, dim)\n",
    "        \n",
    "        self.pos_emb = Linear(3, 5)\n",
    "        self.atom_emb = Embedding(5, 5)\n",
    "        self.edge_emb = Embedding(8, 5)\n",
    "        self.dist_emb = Linear(1, 5)\n",
    "        nn = Sequential(Linear(10, 128), LeakyReLU(0.1), Linear(128, dim * dim))\n",
    "        self.conv = NNConv(dim, dim, nn, aggr='mean')\n",
    "        self.gru = GRU(dim, dim)\n",
    "\n",
    "        self.set2set = Set2Set(dim, processing_steps=6)\n",
    "        self.lin1 = torch.nn.Linear(2 * dim, dim)\n",
    "        self.lin2 = torch.nn.Linear(dim, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        pos = self.pos_emb(data.pos)\n",
    "        atom_emb = self.atom_emb(data.atom)\n",
    "        x = torch.cat((pos, atom_emb), dim=1)\n",
    "        out = F.relu(self.lin0(x))\n",
    "        h = out.unsqueeze(0)\n",
    "        \n",
    "        edge_emb = self.edge_emb(data.edge_types)\n",
    "        edge_dist = self.dist_emb(data.edge_attr)\n",
    "        edge_attr = torch.cat((edge_emb, edge_dist), dim=1)\n",
    "\n",
    "        for i in range(6):\n",
    "            m = F.relu(self.conv(out, data.edge_index, edge_attr))\n",
    "            out, h = self.gru(m.unsqueeze(0), h)\n",
    "            out = out.squeeze(0)\n",
    "            \n",
    "        x = torch.index_select(out, 0, data.edge_index.T.contiguous().view(-1))\n",
    "        x = x.view((data.edge_index.shape[1], -1))\n",
    "\n",
    "        #out = self.set2set(out, data.batch)\n",
    "        out = F.relu(self.lin1(x))\n",
    "        out = self.lin2(out)\n",
    "        return out.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.batch_edge_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-5, weight_decay=1e-5)\n",
    "net.train()\n",
    "net = net.to('cuda')\n",
    "\n",
    "mb = master_bar(range(50))\n",
    "losses = deque(maxlen=400)\n",
    "v_losses = deque(maxlen=400)\n",
    "idx = 0\n",
    "tv_loader = list(zip())\n",
    "num_train = train_mask.sum().item()\n",
    "for epoch in mb:\n",
    "    e_losses = []\n",
    "    for batch in progress_bar(train_loader, parent=mb):\n",
    "        optimizer.zero_grad()\n",
    "        batch = batch.to('cuda')\n",
    "        out = net(batch)\n",
    "        loss = F.mse_loss(out, batch.scalar_coupling)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        mb.child.comment = f'Loss = {loss.item()}'\n",
    "        e_losses.append([loss.item() * batch.num_graphs])\n",
    "        losses.append(loss.item())\n",
    "        mb.update_graph([[range(len(losses)), losses],\n",
    "                         [range(len(v_losses)), v_losses]])\n",
    "    \n",
    "    # Validate\n",
    "    val_loss = []\n",
    "    cnt = 0\n",
    "    with torch.no_grad():\n",
    "        for b in progress_bar(valid_loader, parent=mb):\n",
    "            pred = net(b.to('cuda'))\n",
    "            loss = F.mse_loss(pred, b.scalar_coupling)\n",
    "            v_losses.append(loss.item())\n",
    "            val_loss.append(loss.item() * b.num_graphs)\n",
    "            mb.update_graph([[range(len(losses)), losses],\n",
    "                             [range(len(v_losses)), v_losses]])\n",
    "            cnt += b.num_graphs\n",
    "    valid_loss = np.sum(val_loss) / cnt\n",
    "    \n",
    "    mb.write(f'Finished epoch {epoch}, Loss: {np.sum(e_losses) / num_train}, Valid Loss: {valid_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net, 'model_0.0360_0.03757.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
